{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "10-Regression-nn-solution.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCmoPU78ep42"
      },
      "source": [
        "# Neural Network Regression task - Bike sharing\n",
        "\n",
        "Bike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return has become automatic. Through these systems, a user is able to easily rent a bike from a particular position and return it at another place.\n",
        "\n",
        "The dataset contains the hourly count of rental bikes between years 2011 and 2012 in the Capital Bikeshare system (Wasington DC) with the corresponding weather and seasonal information.\n",
        "\n",
        "The goal of this task is to train a regressor to predict total counts of bike rentals based on the provided features for a given hour. \n",
        "\n",
        "## Data source\n",
        "[http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset)\n",
        "\n",
        "## Feature description\n",
        "* **dteday** - date time stamot\n",
        "* **season** - season (1: spring, 2: summer, 3: fall, 4: winter)\n",
        "* **yr** - year (0: 2011, 1: 2012)\n",
        "* **mnth** - month (1 to 12)\n",
        "* **hr** - hour (0 to 23)\n",
        "* **holiday** - 1 if the day is a holiday, else 0 (extracted from [holiday schedules](https://dchr.dc.gov/page/holiday-schedules))\n",
        "* **weekday** - day of the week (0 to 6)\n",
        "* **workingday** - is 1 if day is neither weekend nor holiday, else 0.\n",
        "* **weathersit** \n",
        "    * 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
        "    * 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
        "    * 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
        "    * 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
        "* **temp** - Normalized temperature in degrees of Celsius.\n",
        "* **atemp** - Normalized feeling temperature in degrees Celsius.\n",
        "* **hum** - Normalized relative humidity.\n",
        "* **windspeed** - Normalized wind speed.\n",
        "* **casual** - Count of casual users.\n",
        "* **registered** - Count of registered users.\n",
        "* **cnt** -  Count of total rental bikes including both casual and registered. This is the target value. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ywJ31Avep46"
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/mlcollege/introduction-to-ml/master/data/bikes.csv', sep=',')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xokWamQyep5D"
      },
      "source": [
        "## Add some features from the past\n",
        "\n",
        "Since we have time stamp of every measurement, we can see the data as a time series and use data from the past. We add one feature column computed from the data of the previous hour."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyh8au_7ep5G"
      },
      "source": [
        "data.sort_values(['dteday', 'hr'])\n",
        "cnt = data['cnt']\n",
        "data['hist'] = cnt.shift(1)\n",
        "data = data[1:]\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w0XNX95ep5J"
      },
      "source": [
        "## Neural Network regressor\n",
        "\n",
        "Implement a neural network regressor based on all reasonable features from the input data set. Notice that some of the features from the input data cannot be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uZvKUTjep5K"
      },
      "source": [
        "### Data preparation\n",
        "\n",
        "Prepare train and test data sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itYtdybpep5L"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_all = data[['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit','temp', 'atemp', 'hum', 'windspeed', 'hist']]\n",
        "y_all = data['cnt']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_all, \n",
        "    y_all,\n",
        "    random_state=1,\n",
        "    test_size=0.2)\n",
        "\n",
        "print('Train size: {}'.format(len(X_train)))\n",
        "print('Test size: {}'.format(len(X_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhdMEq0Yep5O"
      },
      "source": [
        "Standardize the features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lSgMQN6ep5O"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llBvnJeUep5R"
      },
      "source": [
        "### Training a regressor\n",
        "Design and train a regression model. Use the [mean squared error](https://keras.io/losses/) loss function. Experiment with various architectures, [activation functions](https://keras.io/activations/) and [optimizers](https://keras.io/optimizers/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gnjl0_2Mep5T"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(32, input_shape=(13, )))\n",
        "model.add(Activation('tanh'))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(Dense(32))\n",
        "model.add(Activation('tanh'))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5WwqCaTep5V"
      },
      "source": [
        "Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIw7Vbapep5W"
      },
      "source": [
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer='adam',\n",
        "              metrics=['mae'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-JXk8UQep5Z"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apnhGmzMep5a"
      },
      "source": [
        "model.fit(X_train, y_train,\n",
        "          batch_size = 128, epochs = 500, verbose=1,\n",
        "          validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmCYv3B3ep5c"
      },
      "source": [
        "### Evaluate the models\n",
        "\n",
        "Measure mean squared error and mean absolute error evaluation metrics on both train and test data sets. Compute the mean and standard deviation of the target values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EuBphCRep5d"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print (\"Test mean: {}, std: {}\".format(np.mean(y_test), np.std(y_test)))\n",
        "print(\"Test Root mean squared error: {:.2f}\".format(np.sqrt(mean_squared_error(y_test, y_pred))))\n",
        "print(\"Test Mean absolute error: {:.2f}\".format(mean_absolute_error(y_test, y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYC4vyfZep5j"
      },
      "source": [
        "y_pred = model.predict(X_train)\n",
        "print(\"Train Root mean squared error: %.2f\"\n",
        "      % np.sqrt(mean_squared_error(y_train, y_pred)))\n",
        "print(\"Train Mean absolute error: %.2f\"\n",
        "      % mean_absolute_error(y_train, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}